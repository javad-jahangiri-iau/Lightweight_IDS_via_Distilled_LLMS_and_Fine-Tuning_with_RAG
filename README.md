"# Lightweight_IDS_via_Distilled_LLMS_and_Fine-Tuning_with_RAG" 
Abstract
Intrusion Detection Systems (IDS) serve an essential role in protecting networks and systems from attacks and unauthorized access
 in cybersecurity practice. Nevertheless, running an IDS on resource-limited firewall hardware is very undesirable due to the high
 processing requirements of conventional models. This paper presented an IDS optimized for this kind of case. We utilize Distilled
 Language Models (DLMs) as compact yet powerful alternatives to Large Language Models (LLMs). The integration of Fine-Tuning and
 Retrieval-Augmented Generation (RAG) helps DLMs to outperform. We use DLMs and fine-tune DLMs on datasets related to IDS.
 We incorporate RAG to support models' operations by leveraging knowledge base retrievals, thereby increasing accuracy 
 without significantly increasing computational requirements. It performs evaluations across
 various datasets, including NSL-KDD, CICIDS, and UNSW-NB, and demonstrates better detection
 accuracy, resource utilization, and adaptability to diverse attack cases compared to previous
 works. Experimental results demonstrate that hybridized versions of distilled
 models, fine-tuning, and RAG can serve as an operational solution for deploying an efficacious version 
 of an IDS in resource-constrained hardware, providing an avenue for enhanced security in embedded 
 systems and Internet of Things (IoT) appliances.
Keywords: Intrusion Detection System, Distilled Language Models, 
Fine-Tuning, Retrieval-Augmented Generation, Resource-Constrained Hardware.
